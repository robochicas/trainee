[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "aulas/Aula 1.html",
    "href": "aulas/Aula 1.html",
    "title": "Conceitos Importantes",
    "section": "",
    "text": "Nesta aula voc√™ aprender√° os conceitos fundamentais de programa√ß√£o, incluindo o que s√£o programas, l√≥gica de programa√ß√£o, algoritmos, pseudoc√≥digo e vari√°veis. Estes conceitos s√£o essenciais para come√ßar sua jornada em desenvolvimento de software.",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Conceitos Importantes"
    ]
  },
  {
    "objectID": "aulas/Aula 1.html#v√≠deo-de-apoio-geral",
    "href": "aulas/Aula 1.html#v√≠deo-de-apoio-geral",
    "title": "Conceitos Importantes",
    "section": "V√≠deo de Apoio Geral",
    "text": "V√≠deo de Apoio Geral",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Conceitos Importantes"
    ]
  },
  {
    "objectID": "aulas/Aula 1.html#programa",
    "href": "aulas/Aula 1.html#programa",
    "title": "Conceitos Importantes",
    "section": "1.1 Programa",
    "text": "1.1 Programa\n√â um conjunto de instru√ß√µes que descrevem uma tarefa a ser realizada pelo computador.",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Conceitos Importantes"
    ]
  },
  {
    "objectID": "aulas/Aula 1.html#l√≥gica-de-programa√ß√£o",
    "href": "aulas/Aula 1.html#l√≥gica-de-programa√ß√£o",
    "title": "Conceitos Importantes",
    "section": "1.2 L√≥gica de Programa√ß√£o",
    "text": "1.2 L√≥gica de Programa√ß√£o\nA l√≥gica de programa√ß√£o √© a organiza√ß√£o de ideias e instru√ß√µes de forma l√≥gica, com o objetivo de resolver problemas ou realizar tarefas de maneira automatizada. Ou seja, √© o racioc√≠nio usado para escrever um c√≥digo que o computador consiga entender. Com a l√≥gica de programa√ß√£o, conseguimos pensar passo a passo em uma solu√ß√£o e depois transform√°-la em instru√ß√µes.",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Conceitos Importantes"
    ]
  },
  {
    "objectID": "aulas/Aula 1.html#algoritmos",
    "href": "aulas/Aula 1.html#algoritmos",
    "title": "Conceitos Importantes",
    "section": "1.3 Algoritmos",
    "text": "1.3 Algoritmos\nOs algoritmos s√£o uma sequ√™ncia de passo a passo, com a finalidade de resolver um problema ou executar uma tarefa.",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Conceitos Importantes"
    ]
  },
  {
    "objectID": "aulas/Aula 1.html#pseudoc√≥digo",
    "href": "aulas/Aula 1.html#pseudoc√≥digo",
    "title": "Conceitos Importantes",
    "section": "1.4 Pseudoc√≥digo",
    "text": "1.4 Pseudoc√≥digo\nO pseudoc√≥digo √© uma forma de escrever um algoritmo usando uma linguagem similar √† linguagem humana. Ela serve para planejar o racioc√≠nio antes de programar.",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Conceitos Importantes"
    ]
  },
  {
    "objectID": "aulas/Aula 1.html#vari√°veis-e-constantes",
    "href": "aulas/Aula 1.html#vari√°veis-e-constantes",
    "title": "Conceitos Importantes",
    "section": "1.5 Vari√°veis e Constantes",
    "text": "1.5 Vari√°veis e Constantes\nA vari√°vel √© um objeto que armazena os dados que precisamos no algoritmo, e o seu valor pode ser alterado durante a execu√ß√£o do programa. J√° a constante, √© uma valor que √© pr√© definido no algoritmo. Ou seja, ela n√£o muda na execu√ß√£o do programa.",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Conceitos Importantes"
    ]
  },
  {
    "objectID": "aulas/Aula 9.html",
    "href": "aulas/Aula 9.html",
    "title": "Bonus: Data ethics",
    "section": "",
    "text": "Because deep learning is such a powerful tool and can be used for so many things, it becomes particularly important that we consider the consequences of our choices. The philosophical study of ethics is the study of right and wrong, including how we can define those terms, recognize right and wrong actions, and understand the connection between actions and consequences.\nThe field of data ethics has been around for a long time, and there are many academics focused on this field. It is being used to help define policy in many jurisdictions; it is being used in companies big and small to consider how best to ensure good societal outcomes from product development; and it is being used by researchers who want to make sure that the work they are doing is used for good, and not for bad.",
    "crumbs": [
      "Home",
      "Pilar 3",
      "Bonus: Data ethics"
    ]
  },
  {
    "objectID": "aulas/Aula 9.html#video",
    "href": "aulas/Aula 9.html#video",
    "title": "Bonus: Data ethics",
    "section": "Video",
    "text": "Video\n\n\nThis lesson, taught by Dr Rachel Thomas, the founding director of the Center for Applied Data Ethics at the University of San Francisco, was recorded in 2020 during the previous iteration of this course. It discusses some useful ways of thinking about data ethics, particularly through the lens of a number of case studies. It is based partly on chapter 3 of the book.",
    "crumbs": [
      "Home",
      "Pilar 3",
      "Bonus: Data ethics"
    ]
  },
  {
    "objectID": "aulas/Aula 9.html#useful-links",
    "href": "aulas/Aula 9.html#useful-links",
    "title": "Bonus: Data ethics",
    "section": "Useful links",
    "text": "Useful links\n\nDatasheets for datasets\nWeapons of math destruction\nAI Generated Faces\nPapers/repos/tools on how to check for bias\nMarkkula Center - Ethical Toolkit\nThe Pollyannish Assumption\nUnderstanding the context and consequences of pre-trial detention\nFast.ai community ethics resources & discussion\nMontreal AI Ethics Institute Weekly Newsletter",
    "crumbs": [
      "Home",
      "Pilar 3",
      "Bonus: Data ethics"
    ]
  },
  {
    "objectID": "aulas/Aula 7.html",
    "href": "aulas/Aula 7.html",
    "title": "7: Collaborative filtering",
    "section": "",
    "text": "You interact nearly every day with recommendation systems‚Äîalgorithms which guess what products and services you might like, based on your past behavior. These systems largely rely on collaborative-filtering, an approach based on linear algebra that fills in the missing values in a matrix. Today we‚Äôll see two ways to do this: one based on a classic linear algebra formulation, and one based on deep learning.",
    "crumbs": [
      "Home",
      "Pilar 2",
      "7: Collaborative filtering"
    ]
  },
  {
    "objectID": "aulas/Aula 7.html#video",
    "href": "aulas/Aula 7.html#video",
    "title": "7: Collaborative filtering",
    "section": "Video",
    "text": "Video\n\n\nThis lesson is based partly on chapter 8 of the book.",
    "crumbs": [
      "Home",
      "Pilar 2",
      "7: Collaborative filtering"
    ]
  },
  {
    "objectID": "aulas/Aula 7.html#resources",
    "href": "aulas/Aula 7.html#resources",
    "title": "7: Collaborative filtering",
    "section": "Resources",
    "text": "Resources\n\nNotebooks for this lesson:\n\nRoad to the top: part 3 and part 4\nCollaborative Filtering Deep Dive\n\nSpreadsheets for this lesson:\n\nSoftmax and cross-entropy\nCollaborative filterings and embeddings\n\nThings that confused me about cross-entropy by Chris Said\nLabel Smoothing Explained using Microsoft Excel by Aman Arora",
    "crumbs": [
      "Home",
      "Pilar 2",
      "7: Collaborative filtering"
    ]
  },
  {
    "objectID": "aulas/Aula 8.html",
    "href": "aulas/Aula 8.html",
    "title": "8: Convolutions (CNNs)",
    "section": "",
    "text": "Today we finish off our study of collaborative filtering by looking closely at embeddings‚Äîa critical building block of many deep learning algorithms. Then we‚Äôll dive into convolutional neural networks (CNNs) and see how they really work. We‚Äôve used plenty of CNNs through this course, but we haven‚Äôt peeked inside them to see what‚Äôs really going on in there. As well as learning about their most fundamental building block, the convolution, we‚Äôll also look at pooling, dropout, and more.",
    "crumbs": [
      "Home",
      "Pilar 2",
      "8: Convolutions (CNNs)"
    ]
  },
  {
    "objectID": "aulas/Aula 8.html#video",
    "href": "aulas/Aula 8.html#video",
    "title": "8: Convolutions (CNNs)",
    "section": "Video",
    "text": "Video\n\n\nThis lesson is based partly on chapter 13 of the book.",
    "crumbs": [
      "Home",
      "Pilar 2",
      "8: Convolutions (CNNs)"
    ]
  },
  {
    "objectID": "aulas/Aula 8.html#resources",
    "href": "aulas/Aula 8.html#resources",
    "title": "8: Convolutions (CNNs)",
    "section": "Resources",
    "text": "Resources\n\nNotebooks for this lesson\n\nCollaborative Filtering Deep Dive\n\nSpreadsheets for this lesson\n\nCollaborative filterings and embeddings\nConvolutions\n\nOther resources for the lesson\n\nPlease add any questions you want Jeremy to answer to the AMA thread ‚Äì and upvote any there you‚Äôre interested in\nSpecial extra: Data ethics lesson\n\nSolutions to chapter 8 questionnaire from the book",
    "crumbs": [
      "Home",
      "Pilar 2",
      "8: Convolutions (CNNs)"
    ]
  },
  {
    "objectID": "aulas/Aula 10.html",
    "href": "aulas/Aula 10.html",
    "title": "10: Diving Deeper",
    "section": "",
    "text": "This lesson creates a complete Diffusers pipeline from the underlying components: the VAE, unet, scheduler, and tokeniser. By putting them together manually, this gives you the flexibility to fully customise every aspect of the inference process.\nWe also discuss three important new papers that have been released in the last week, which improve inference performance by over 10x, and allow any photo to be ‚Äúedited‚Äù by just describing what the new picture should show.\nIn the second half of the lesson Jeremy begins the ‚Äúfrom scratch‚Äù implementation of Stable Diffusion. He introduces the ‚Äúminiai‚Äù library which will be created by students during the course, and discusses organising and simplifying code. The lesson discusses the Python data model, tensors, and random number generation. Jeremy introduces the Wickman-Hill random number generation algorithm and compares the performance of custom and Pytorch‚Äôs built-in random number generators. The lesson concludes with creating a linear classifier using a tensor.",
    "crumbs": [
      "Home",
      "Pilar 3",
      "10: Diving Deeper"
    ]
  },
  {
    "objectID": "aulas/Aula 10.html#concepts-discussed",
    "href": "aulas/Aula 10.html#concepts-discussed",
    "title": "10: Diving Deeper",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nPapers:\n\nProgressive Distillation for Fast Sampling of Diffusion Models\nOn Distillation of Guided Diffusion Models\nImagic\n\nTokenizing input text\nCLIP encoder for embeddings\nScheduler for noise determination\nOrganizing and simplifying code\nNegative prompts and callbacks\nIterators and generators in Python\nCustom class for matrices\nDunder methods\nPython data model\nTensors\nPseudo-random number generation\n\nWickman-Hill algorithm\nRandom state in deep learning\n\nLinear classifier using a tensor",
    "crumbs": [
      "Home",
      "Pilar 3",
      "10: Diving Deeper"
    ]
  },
  {
    "objectID": "aulas/Aula 10.html#video",
    "href": "aulas/Aula 10.html#video",
    "title": "10: Diving Deeper",
    "section": "Video",
    "text": "Video",
    "crumbs": [
      "Home",
      "Pilar 3",
      "10: Diving Deeper"
    ]
  },
  {
    "objectID": "aulas/Aula 10.html#lesson-resources",
    "href": "aulas/Aula 10.html#lesson-resources",
    "title": "10: Diving Deeper",
    "section": "Lesson resources",
    "text": "Lesson resources\n\nDiscuss this lesson\nPaper walkthrough video by @johnowhitaker covering Progressive Distillation for Fast Sampling of Diffusion Models\ndiffusion-nbs repo (we continue walking through stable_diffusion.ipynb that we touched upon last time)\nFashion-MNIST reimplementation of the lesson, with notes, by @strickvl",
    "crumbs": [
      "Home",
      "Pilar 3",
      "10: Diving Deeper"
    ]
  },
  {
    "objectID": "aulas/Aula 10.html#links-from-the-lesson",
    "href": "aulas/Aula 10.html#links-from-the-lesson",
    "title": "10: Diving Deeper",
    "section": "Links from the lesson",
    "text": "Links from the lesson\n\nCourse 2022p2 repo\nProgressive Distillation for Fast Sampling of Diffusion Models\nImagic paper. Within a few hours stable diffusion versions are appearing.\nAPL: Array programming - fast.ai Course Forums",
    "crumbs": [
      "Home",
      "Pilar 3",
      "10: Diving Deeper"
    ]
  },
  {
    "objectID": "aulas/Aula 11.html",
    "href": "aulas/Aula 11.html",
    "title": "11: Matrix multiplication",
    "section": "",
    "text": "In this lesson, we discuss various techniques and experiments shared by students on the forum, such as interpolating between prompts for visually appealing transitions and improving the update process in text-to-image generation, and a novel approach to decreasing the guidance scale during image generation. We then dive into a new paper called DiffEdit, which focuses on semantic image editing using text-conditioned diffusion models. We walk through the process of reading and understanding the paper, emphasizing the importance of grasping the main idea and not getting bogged down in every detail.\nWe then embark on a deep exploration of matrix multiplication using Python, compare APL with PyTorch, and introduce the concept of Frobenius norm. We also discuss the powerful concept of broadcasting, which allows for operations between tensors of different shapes, and demonstrate its efficiency in speeding up matrix multiplication. The techniques introduced in this lesson allow us to speed up our initial Python implementation by a factor of around five million, including leveraging the GPU for massive parallelism!",
    "crumbs": [
      "Home",
      "Pilar 3",
      "11: Matrix multiplication"
    ]
  },
  {
    "objectID": "aulas/Aula 11.html#concepts-discussed",
    "href": "aulas/Aula 11.html#concepts-discussed",
    "title": "11: Matrix multiplication",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nDiffusion improvements\n\nInterpolating between prompts for visually appealing transitions\nImproving the update process in text-to-image generation\nDecreasing the guidance scale during image generation\n\nUnderstanding research papers\nMatrix multiplication using Python and Numba\nComparing APL with PyTorch\nFrobenius norm\nBroadcasting in deep learning and machine learning code",
    "crumbs": [
      "Home",
      "Pilar 3",
      "11: Matrix multiplication"
    ]
  },
  {
    "objectID": "aulas/Aula 11.html#video",
    "href": "aulas/Aula 11.html#video",
    "title": "11: Matrix multiplication",
    "section": "Video",
    "text": "Video",
    "crumbs": [
      "Home",
      "Pilar 3",
      "11: Matrix multiplication"
    ]
  },
  {
    "objectID": "aulas/Aula 11.html#lesson-resources",
    "href": "aulas/Aula 11.html#lesson-resources",
    "title": "11: Matrix multiplication",
    "section": "Lesson resources",
    "text": "Lesson resources\n\nDiscuss this lesson\nDiffEdit: Diffusion-based semantic image editing with mask guidance\nMath notation\n\nGreek letters\nAll in one mathematics cheat sheet (PDF)\nGlossary of mathematical symbols (wikipedia)\npix2tex (open source) or Mathpix (commercial)\nGreek Letters for Deep Learning - Anki deck containing fastai-related Greek letters\nDetexify Draw math symbols",
    "crumbs": [
      "Home",
      "Pilar 3",
      "11: Matrix multiplication"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Os Tr√™s pilares da rob√≥tica",
    "section": "",
    "text": "Ol√°, Chicas ! Este pequeno material tem como objetivo introduzir voc√™s no universo da robotica, trazendo conceitos iniciais que servem como bases para seus estudos. Encarem este conte√∫do como um guia de partida, com explica√ß√µes simples e objetivas, al√©m de links para v√≠deos que ir√£o auxili√°-las no aprendizado. Claro, voc√™s t√™m toda a liberdade para explorar outros materiais complementares que julgarem √∫teis, o importante √© entender de verdade! E acreditamos que explorar outros materiais, buscar fontes diferentes e aprender a aprender s√£o passos essenciais para o crescimento. Fiquem √† vontade para complementar seus estudos com o que acharem interessante.\nDesejamos a voc√™s uma √≥tima jornada de aprendizado. Que esse seja o primeiro passo de muitos!",
    "crumbs": [
      "Home",
      "Os Tr√™s pilares da rob√≥tica"
    ]
  },
  {
    "objectID": "index.html#video",
    "href": "index.html#video",
    "title": "Os Tr√™s pilares da rob√≥tica",
    "section": "Video",
    "text": "Video",
    "crumbs": [
      "Home",
      "Os Tr√™s pilares da rob√≥tica"
    ]
  },
  {
    "objectID": "index.html#recursos",
    "href": "index.html#recursos",
    "title": "Os Tr√™s pilares da rob√≥tica",
    "section": "Recursos üìπ",
    "text": "Recursos üìπ\n\nPlaylist completa de l√≥gica de programa√ß√£o\nPlaylist: Conceitos B√°sicos de Eletr√¥nica\nMaterial complementar sobre algoritmos",
    "crumbs": [
      "Home",
      "Os Tr√™s pilares da rob√≥tica"
    ]
  },
  {
    "objectID": "aulas/Aula 5.html",
    "href": "aulas/Aula 5.html",
    "title": "5: From-scratch model",
    "section": "",
    "text": "Today we look at how to create a neural network from scratch using Python and PyTorch, and how to implement a training loop for optimising the weights of a model. We build up from a single layer regression model up to a neural net with one hidden layer, and then to a deep learning model. Along the way we‚Äôll also look at how we can use a special function called sigmoid to make binary classification models easier to train, and we‚Äôll also learn about metrics.",
    "crumbs": [
      "Home",
      "Pilar 2",
      "5: From-scratch model"
    ]
  },
  {
    "objectID": "aulas/Aula 5.html#video",
    "href": "aulas/Aula 5.html#video",
    "title": "5: From-scratch model",
    "section": "Video",
    "text": "Video\n\n\nThis lesson is based partly on chapter 4 and chapter 9 of the book.",
    "crumbs": [
      "Home",
      "Pilar 2",
      "5: From-scratch model"
    ]
  },
  {
    "objectID": "aulas/Aula 5.html#lesson-notebooks",
    "href": "aulas/Aula 5.html#lesson-notebooks",
    "title": "5: From-scratch model",
    "section": "Lesson notebooks",
    "text": "Lesson notebooks\n\nLinear model and neural net from scratch\nWhy you should use a framework\nHow random forests really work",
    "crumbs": [
      "Home",
      "Pilar 2",
      "5: From-scratch model"
    ]
  },
  {
    "objectID": "aulas/Aula 5.html#links-from-the-lesson",
    "href": "aulas/Aula 5.html#links-from-the-lesson",
    "title": "5: From-scratch model",
    "section": "Links from the lesson",
    "text": "Links from the lesson\n\nOneR paper\nSome great Titanic notebooks: 1; 2; 3; 4",
    "crumbs": [
      "Home",
      "Pilar 2",
      "5: From-scratch model"
    ]
  },
  {
    "objectID": "aulas/Aula 6.html",
    "href": "aulas/Aula 6.html",
    "title": "6: Random forests",
    "section": "",
    "text": "Random forests started a revolution in machine learning 20 years ago. For the first time, there was a fast and reliable algorithm which made almost no assumptions about the form of the data, and required almost no preprocessing. In today‚Äôs lesson, you‚Äôll learn how a random forest really works, and how to build one from scratch. And, just as importantly, you‚Äôll learn how to interpret random forests to better understand your data.",
    "crumbs": [
      "Home",
      "Pilar 2",
      "6: Random forests"
    ]
  },
  {
    "objectID": "aulas/Aula 6.html#video",
    "href": "aulas/Aula 6.html#video",
    "title": "6: Random forests",
    "section": "Video",
    "text": "Video\n\n\nThis lesson is based partly on chapter 9 of the book.",
    "crumbs": [
      "Home",
      "Pilar 2",
      "6: Random forests"
    ]
  },
  {
    "objectID": "aulas/Aula 6.html#lesson-notebooks",
    "href": "aulas/Aula 6.html#lesson-notebooks",
    "title": "6: Random forests",
    "section": "Lesson notebooks",
    "text": "Lesson notebooks\n\nHow random forests really work\nRoad to the top, part 1",
    "crumbs": [
      "Home",
      "Pilar 2",
      "6: Random forests"
    ]
  },
  {
    "objectID": "aulas/Aula 6.html#links-from-the-lesson",
    "href": "aulas/Aula 6.html#links-from-the-lesson",
    "title": "6: Random forests",
    "section": "Links from the lesson",
    "text": "Links from the lesson\n\nHow to explain Gradient Boosting\n‚ÄúStatistical Modeling: The Two Cultures‚Äù by Leo Breiman",
    "crumbs": [
      "Home",
      "Pilar 2",
      "6: Random forests"
    ]
  },
  {
    "objectID": "aulas/Aula 2.html",
    "href": "aulas/Aula 2.html",
    "title": "Operadores",
    "section": "",
    "text": "Nesta aula voc√™ aprender√° sobre os diferentes tipos de operadores em programa√ß√£o, incluindo operadores relacionais, aritm√©ticos e l√≥gicos. Estes s√£o fundamentais para realizar compara√ß√µes, c√°lculos e combina√ß√µes l√≥gicas em seus programas.",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Operadores"
    ]
  },
  {
    "objectID": "aulas/Aula 2.html#introdu√ß√£o-aos-operadores",
    "href": "aulas/Aula 2.html#introdu√ß√£o-aos-operadores",
    "title": "Operadores",
    "section": "Introdu√ß√£o aos Operadores",
    "text": "Introdu√ß√£o aos Operadores\nOs operadores s√£o s√≠mbolos usados para realizar opera√ß√µes, como c√°lculos e compara√ß√µes. Os tipos principais de operadores s√£o:",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Operadores"
    ]
  },
  {
    "objectID": "aulas/Aula 2.html#operadores-relacionais",
    "href": "aulas/Aula 2.html#operadores-relacionais",
    "title": "Operadores",
    "section": "2.1 Operadores Relacionais",
    "text": "2.1 Operadores Relacionais\nOs operadores relacionais s√£o utilizados para comparar dois valores.\n\n\n\nOperador\nSignificado\n\n\n\n\n&gt;\nMaior que\n\n\n&lt;\nMenor que\n\n\n&gt;=\nMaior ou igual\n\n\n&lt;=\nMenor ou igual\n\n\n==\nIgual a\n\n\n!=\nDiferente de",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Operadores"
    ]
  },
  {
    "objectID": "aulas/Aula 2.html#operadores-aritm√©ticos",
    "href": "aulas/Aula 2.html#operadores-aritm√©ticos",
    "title": "Operadores",
    "section": "2.2 Operadores Aritm√©ticos",
    "text": "2.2 Operadores Aritm√©ticos\nOs operadores aritm√©ticos s√£o utilizados para realizar opera√ß√µes matem√°ticas.\n\n\n\nOperador\nOpera√ß√£o\n\n\n\n\n+\nAdi√ß√£o\n\n\n-\nSubtra√ß√£o\n\n\n*\nMultiplica√ß√£o\n\n\n/\nDivis√£o\n\n\n%\nResto da divis√£o\n\n\n**\nExponencia√ß√£o",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Operadores"
    ]
  },
  {
    "objectID": "aulas/Aula 2.html#operadores-l√≥gicos",
    "href": "aulas/Aula 2.html#operadores-l√≥gicos",
    "title": "Operadores",
    "section": "2.3 Operadores L√≥gicos",
    "text": "2.3 Operadores L√≥gicos\nOs operadores l√≥gicos s√£o utilizados para combinar condi√ß√µes.\n\n\n\nOperador\nSignificado\n\n\n\n\nAND (&&)\nE l√≥gico\n\n\nOR (\n\n\n\nNOT (!)\nN√ÉO l√≥gico",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Operadores"
    ]
  },
  {
    "objectID": "aulas/Aula 4.html",
    "href": "aulas/Aula 4.html",
    "title": "Estruturas de Repeti√ß√£o",
    "section": "",
    "text": "Nesta aula voc√™ aprender√° sobre estruturas de repeti√ß√£o, que s√£o usadas quando precisamos executar um bloco de c√≥digo v√°rias vezes. Voc√™ conhecer√° os loops FOR e WHILE, entendendo quando usar cada um deles.",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Estruturas de Repeti√ß√£o"
    ]
  },
  {
    "objectID": "aulas/Aula 4.html#introdu√ß√£o-√†s-estruturas-de-repeti√ß√£o",
    "href": "aulas/Aula 4.html#introdu√ß√£o-√†s-estruturas-de-repeti√ß√£o",
    "title": "Estruturas de Repeti√ß√£o",
    "section": "Introdu√ß√£o √†s Estruturas de Repeti√ß√£o",
    "text": "Introdu√ß√£o √†s Estruturas de Repeti√ß√£o\nAs estruturas de repeti√ß√£o s√£o fundamentais quando precisamos executar um bloco de c√≥digo m√∫ltiplas vezes de forma automatizada.",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Estruturas de Repeti√ß√£o"
    ]
  },
  {
    "objectID": "aulas/Aula 4.html#loop-for",
    "href": "aulas/Aula 4.html#loop-for",
    "title": "Estruturas de Repeti√ß√£o",
    "section": "4.1 Loop FOR",
    "text": "4.1 Loop FOR\nO loop FOR √© utilizado quando sabemos previamente o n√∫mero de repeti√ß√µes que queremos executar.",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Estruturas de Repeti√ß√£o"
    ]
  },
  {
    "objectID": "aulas/Aula 4.html#loop-while",
    "href": "aulas/Aula 4.html#loop-while",
    "title": "Estruturas de Repeti√ß√£o",
    "section": "4.2 Loop WHILE",
    "text": "4.2 Loop WHILE\nO loop WHILE continua executando enquanto uma determinada condi√ß√£o for verdadeira.",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Estruturas de Repeti√ß√£o"
    ]
  },
  {
    "objectID": "aulas/Aula 13.html",
    "href": "aulas/Aula 13.html",
    "title": "9: Stable Diffusion",
    "section": "",
    "text": "Here‚Äôs what you need to know to complete this course:\n\nThe lesson is presented as a video, which you can jump directly to by clicking the table of contents on the right\nEach video goes through one or more Jupyter notebooks, which you‚Äôll need to run and experiment with to get the most out of the course\nAll information needed to complete a lesson (including links to the repo with the notebooks) is in the ‚Äúlesson resources‚Äù section of the lesson page\nAmongst the lesson resources you‚Äôll find a ‚Äúdiscuss this lesson‚Äù link, which will take you to a Q&A page on our forums for that particular lesson\nThe material covered in this course includes stuff that would normally only be included in post-graduate level programs. We try to present it in the clearest way possible, but you should expect to work hard and put in plenty of hours of study\nWe assume familiarity with the material in part 1 of this course. If you find yourself unsure about some of the foundational deep learning ideas refered to in the lessons, we‚Äôd suggest going back to study the lessons in part 1 that cover those ideas\nIf there‚Äôs mathematical or coding concepts that we use that you‚Äôre not comfortable with, don‚Äôt be afraid to seek out other tutorials to help fill in your gaps\nOn forums.fast.ai there are many other students you can collaborate with, and many folks are looking for study groups or study buddies. Studying in groups has been shown to be more effective for most people than studying alone\nIn many lessons we‚Äôll include a challenge for you to complete, some of which involve trying novel research directions where you‚Äôll be venturing into the academic unknown."
  },
  {
    "objectID": "aulas/Aula 13.html#what-you-need-to-know",
    "href": "aulas/Aula 13.html#what-you-need-to-know",
    "title": "9: Stable Diffusion",
    "section": "",
    "text": "Here‚Äôs what you need to know to complete this course:\n\nThe lesson is presented as a video, which you can jump directly to by clicking the table of contents on the right\nEach video goes through one or more Jupyter notebooks, which you‚Äôll need to run and experiment with to get the most out of the course\nAll information needed to complete a lesson (including links to the repo with the notebooks) is in the ‚Äúlesson resources‚Äù section of the lesson page\nAmongst the lesson resources you‚Äôll find a ‚Äúdiscuss this lesson‚Äù link, which will take you to a Q&A page on our forums for that particular lesson\nThe material covered in this course includes stuff that would normally only be included in post-graduate level programs. We try to present it in the clearest way possible, but you should expect to work hard and put in plenty of hours of study\nWe assume familiarity with the material in part 1 of this course. If you find yourself unsure about some of the foundational deep learning ideas refered to in the lessons, we‚Äôd suggest going back to study the lessons in part 1 that cover those ideas\nIf there‚Äôs mathematical or coding concepts that we use that you‚Äôre not comfortable with, don‚Äôt be afraid to seek out other tutorials to help fill in your gaps\nOn forums.fast.ai there are many other students you can collaborate with, and many folks are looking for study groups or study buddies. Studying in groups has been shown to be more effective for most people than studying alone\nIn many lessons we‚Äôll include a challenge for you to complete, some of which involve trying novel research directions where you‚Äôll be venturing into the academic unknown."
  },
  {
    "objectID": "aulas/Aula 13.html#lesson-overview",
    "href": "aulas/Aula 13.html#lesson-overview",
    "title": "9: Stable Diffusion",
    "section": "Lesson overview",
    "text": "Lesson overview\nThis lesson starts with a tutorial on how to use pipelines in the Diffusers library to generate images. Diffusers is (in our opinion!) the best library available at the moment for image generation. It has many features and is very flexible. We explain how to use its many features, and discuss options for accessing the GPU resources needed to use the library.\nWe talk about some of the nifty tweaks available when using Stable Diffusion in Diffusers, and show how to use them: guidance scale (for varying the amount the prompt is used), negative prompts (for removing concepts from an image), image initialisation (for starting with an existing image), textual inversion (for adding your own concepts to generated images), Dreambooth (an alternative approach to textual inversion).\nThe second half of the lesson covers the key concepts involved in Stable Diffusion:\n\nCLIP embeddings\nThe VAE (variational autoencoder)\nPredicting noise with the unet\nRemoving noise with schedulers\n\nJeremy shows a theoretical foundation for how Stable Diffusion works, using a novel interpretation that shows an easily-understood intuition for the theory. He introduces the concept of finite differencing and analytic derivatives, using an example of training a neural network to identify pixel adjustments to make an image look more like a handwritten digit, and describes how the derivatives of such a model can provide the score needed to provide the basis of a diffusion process that generates handwritten digits.\nThe lesson also covers finite differencing, analytic derivatives, autoencoders, and U-Nets. Jeremy introduces the concept of creating a model that can take a sentence and return a vector of numbers representing the image, using two models: a text encoder and an image encoder. The lesson concludes with a discussion of the similarities between diffusion-based models and deep learning optimizers, suggesting new research directions."
  },
  {
    "objectID": "aulas/Aula 13.html#concepts-discussed",
    "href": "aulas/Aula 13.html#concepts-discussed",
    "title": "9: Stable Diffusion",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nStable Diffusion\nHugging Face‚Äôs Diffusers library\nPre-trained pipelines\nGuidance scale\nNegative prompts\nImage-to-image pipelines\nFinite differencing\nAnalytic derivatives\nAutoencoders\nTextual inversion\nDreambooth\nLatents\nU-Nets\nText encoders and image encoders\nContrastive loss function\nCLIP text encoder\nDeep learning optimizers\nPerceptual loss"
  },
  {
    "objectID": "aulas/Aula 13.html#video",
    "href": "aulas/Aula 13.html#video",
    "title": "9: Stable Diffusion",
    "section": "Video",
    "text": "Video"
  },
  {
    "objectID": "aulas/Aula 13.html#lesson-resources",
    "href": "aulas/Aula 13.html#lesson-resources",
    "title": "9: Stable Diffusion",
    "section": "Lesson resources",
    "text": "Lesson resources\n\nOther Videos\n\nLesson 9A video‚ÄîDeep Dive‚Äîfrom @johnowhitaker (with accompanying notebook)\nLesson 9B video‚ÄîThe Math of Diffusion‚Äîfrom @seem and @ilovescience\n\nJeremy‚Äôs lesson notes\nThe fastai book:\n\nPublished version\nFree notebook version\nSample full chapters\n\nStudent notes ‚Äì Lesson Notes h/t @barnacl"
  },
  {
    "objectID": "aulas/Aula 13.html#links-from-the-lesson",
    "href": "aulas/Aula 13.html#links-from-the-lesson",
    "title": "9: Stable Diffusion",
    "section": "Links from the lesson",
    "text": "Links from the lesson\n\nDiscuss this lesson\nCourse repo\ndiffusion-nbs repo\nHuggingFace Notebooks\nGPU servers\n\nLambda Labs\nPaperspace Gradient\nJarvis Labs\nvast.ai - crowdsourced GPU service\n\nPrompt Engineering\n\nLexica\nPromptHero\nHexo - 10M Images and Prompts\n\nTools and Resources for AI Art\nfastai repo"
  },
  {
    "objectID": "aulas/Aula 13.html#useful-background-on-fast.ai-courses",
    "href": "aulas/Aula 13.html#useful-background-on-fast.ai-courses",
    "title": "9: Stable Diffusion",
    "section": "Useful background on fast.ai courses",
    "text": "Useful background on fast.ai courses\n\nHomework\nSummaries + Things Jeremy Says to do + Qs\nFastai: A Layered API for Deep Learning paper: Information Journal or arxiv or fast.ai\nProviding a Good Education in Deep Learning: fast.ai teaching philosophy\n‚ÄúHow not to do fastai‚Äù\n‚ÄúFastAI Lesson Zero: video + notes‚Äù"
  },
  {
    "objectID": "aulas/Aula 12.html",
    "href": "aulas/Aula 12.html",
    "title": "12: Mean shift clustering",
    "section": "",
    "text": "In this lesson, we start by discussing the CLIP Interrogator, a Hugging Face Spaces Gradio app that generates text prompts for creating CLIP embeddings. We then dive back into matrix multiplication, using Einstein summation notation and torch.einsum to simplify code and improve performance. We explore GPU acceleration with CUDA and Numba, demonstrating how to write a kernel function for matrix multiplication and launch it on the GPU.\nNext up we exercise our tensor programming skills by implementing mean shift clustering, a technique for identifying clusters within a dataset. We create synthetic data, explain the mean shift algorithm, and introduce the Gaussian kernel for penalizing distant points. We implement the mean shift clustering algorithm using PyTorch and discuss the importance of tensor manipulation operations for efficient GPU programming.\nFinally, we optimize the mean shift algorithm using PyTorch and GPUs, demonstrating how to calculate weights, multiply matrices, and sum up points to obtain new data points. We explore the impact of changing batch sizes on performance and encourage viewers to research other clustering algorithms.\nThe lesson concludes with an introduction to calculus, focusing on derivatives and the calculus of infinitesimals.",
    "crumbs": [
      "Home",
      "Pilar 3",
      "12: Mean shift clustering"
    ]
  },
  {
    "objectID": "aulas/Aula 12.html#concepts-discussed",
    "href": "aulas/Aula 12.html#concepts-discussed",
    "title": "12: Mean shift clustering",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nCLIP Interrogator\nInverse problems\nMatrix multiplication\nEinstein summation notation and torch.einsum\nGPU acceleration and CUDA\nNumba\nMean shift clustering\nGaussian kernel\nNorms\nEuclidean distance\nCalculus\n\nDerivatives and Infinitesimals",
    "crumbs": [
      "Home",
      "Pilar 3",
      "12: Mean shift clustering"
    ]
  },
  {
    "objectID": "aulas/Aula 12.html#video",
    "href": "aulas/Aula 12.html#video",
    "title": "12: Mean shift clustering",
    "section": "Video",
    "text": "Video",
    "crumbs": [
      "Home",
      "Pilar 3",
      "12: Mean shift clustering"
    ]
  },
  {
    "objectID": "aulas/Aula 12.html#lesson-resources",
    "href": "aulas/Aula 12.html#lesson-resources",
    "title": "12: Mean shift clustering",
    "section": "Lesson resources",
    "text": "Lesson resources\n\nDiscuss this lesson\nCLIP Interrogator\nEssence of calculus (3blue1brown)",
    "crumbs": [
      "Home",
      "Pilar 3",
      "12: Mean shift clustering"
    ]
  },
  {
    "objectID": "aulas/Aula 3.html",
    "href": "aulas/Aula 3.html",
    "title": "Estruturas de Sele√ß√£o",
    "section": "",
    "text": "Nesta aula voc√™ aprender√° sobre estruturas de sele√ß√£o, que s√£o fundamentais para criar programas que podem tomar decis√µes baseadas em condi√ß√µes. Voc√™ ver√° como usar if/else e switch/case para controlar o fluxo do seu programa.",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Estruturas de Sele√ß√£o"
    ]
  },
  {
    "objectID": "aulas/Aula 3.html#introdu√ß√£o-√†s-estruturas-de-sele√ß√£o",
    "href": "aulas/Aula 3.html#introdu√ß√£o-√†s-estruturas-de-sele√ß√£o",
    "title": "Estruturas de Sele√ß√£o",
    "section": "Introdu√ß√£o √†s Estruturas de Sele√ß√£o",
    "text": "Introdu√ß√£o √†s Estruturas de Sele√ß√£o\n√â uma estrutura usada para tomar decis√µes dentro do programa. Dependendo de uma condi√ß√£o, o programa pode seguir caminhos diferentes.",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Estruturas de Sele√ß√£o"
    ]
  },
  {
    "objectID": "aulas/Aula 3.html#ifelse",
    "href": "aulas/Aula 3.html#ifelse",
    "title": "Estruturas de Sele√ß√£o",
    "section": "3.1 If/Else",
    "text": "3.1 If/Else\nPermite executar um bloco de c√≥digo se uma condi√ß√£o for verdadeira, e outra se for falsa.",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Estruturas de Sele√ß√£o"
    ]
  },
  {
    "objectID": "aulas/Aula 3.html#switchcase",
    "href": "aulas/Aula 3.html#switchcase",
    "title": "Estruturas de Sele√ß√£o",
    "section": "3.2 Switch/Case",
    "text": "3.2 Switch/Case\nUsado para escolher entre v√°rias op√ß√µes.",
    "crumbs": [
      "Home",
      "Pilar 1",
      "Estruturas de Sele√ß√£o"
    ]
  },
  {
    "objectID": "index.html#os-tr√™s-pilares-da-rob√≥tica",
    "href": "index.html#os-tr√™s-pilares-da-rob√≥tica",
    "title": "Os Tr√™s pilares da rob√≥tica",
    "section": "Os Tr√™s pilares da rob√≥tica",
    "text": "Os Tr√™s pilares da rob√≥tica\nAntes de entender se aprofundar no pilares da rob√≥tica, √© importante entender ou relembrar alguns conceitos b√°sicos",
    "crumbs": [
      "Home",
      "Os Tr√™s pilares da rob√≥tica"
    ]
  }
]